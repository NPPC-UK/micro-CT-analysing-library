* __init__.py
* ct_data.py
** CTData
*** __init__
*** get_data

	Returns the dataframe used in this class

*** gather_data

	this function gathers together all
	the data of interest
	@param folder is a starting folder
	@returns tuple of (seed files, rachis files)

*** make_dataframe

	this function returns a dataframe of
	grain parameters and optionally of the rachis top and bottom
	@param grain_files is the output from gather_data
	@param rachis_files is an optional output from gather_data also
	@returns a dataframe of the information pre-joining

*** clean_data

	Following parameters outlined in the
	CT software documentation I remove outliers
	which are known to be errors

*** get_files

	Returns a tuple of grain files and rachis files

*** fix_colnames

	Because Biologists like to give data which are not normalised to any degree
	this function exists to attempt to correct the grouping columns,
	after standardisation https://github.com/SirSharpest/CT_Analysing_Library/issues/2
	this shouldn't be needed anymore, but kept for legacy issues that could arise!

*** join_spikes_by_rachis

	So important part of this function is that we accept that the data is what it is
	that is to say: rtop, rbot and Z are all orientated in the proper direction

	It's main purpose is to join split spikes by rachis nodes identified in the
	analysis process

	@param grain_df is the grain dataframe to take on-board

*** remove_percentile

	This function is targeted at removing a percentile of a dataframe
	it uses a column to decide which to measure against. By default this
	will remove everything above the percentile value

	@param df is the dataframe to manipulate
	@param column is the attribute column to base the removal of
	@param target_percent is the percentage to aim for
	@param bool_below is a default param which if set
	to True will remove values below rather than above percentage

*** get_spike_info

	This function should do something akin to adding additional
	information to the data frame

	@note there is some confusion in the NPPC about whether to use
	folder name or file name as the unique id when this is made into
	end-user software, a toggle should be added to allow this

*** look_up
*** gather_data
*** aggregate_spike_averages

	This will aggregate features (specified by attributes) into their medians
	on a per-spike basis.


	Makes direct changes to the dataframe (self.df)

	@param attributes list of features to average

*** make_plot

	Returns false if plot could not be created for invalid parameters

* graphing.py
** Error
Base class for other exceptions
** InvalidPlot
Except to trigger when a graph is given wrong args
*** percentile_grid
*** qq_grid
*** plot_boxplots
*** plot_histogram

    Simple histogram function

    returns a plot axes

*** check_var_args

    Helper function to fix bad arguments
    before they get used in evaluations

* scratch.py
* statistical_tests.py
*** plot_qqplot
*** test_normality

    https://stackoverflow.com/a/12839537

    Null Hypothesis is that X came from a normal distribution

    which means:
    If the p-val is very small, it means it is
    unlikely that the data came from a normal distribution

    As for chi-square:
    https://biology.stackexchange.com/questions/13486/deciding-between-chi-square-and-t-test
